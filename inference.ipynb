{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad6afed",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109c9918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.5.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.5.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "from qai_hub_models.models.facemap_3dmm.model import MODEL_ASSET_VERSION, MODEL_ID\n",
    "from qai_hub_models.utils.asset_loaders import CachedWebModelAsset\n",
    "from ai_edge_litert.interpreter import Interpreter\n",
    "from PIL import Image\n",
    "import coremltools as ct\n",
    "import os, cv2, numpy as np\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c995007",
   "metadata": {},
   "source": [
    "# Initialize Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7fcf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meanFace.npy': 'assets/meanFace.npy', 'shapeBasis.npy': 'assets/shapeBasis.npy', 'blendShape.npy': 'assets/blendShape.npy', 'face_img.jpg': 'assets/face_img.jpg', 'face_img_fbox.txt': 'assets/face_img_fbox.txt'}\n"
     ]
    }
   ],
   "source": [
    "ASSET_NAMES = [\n",
    "    \"meanFace.npy\",\n",
    "    \"shapeBasis.npy\",\n",
    "    \"blendShape.npy\",\n",
    "    \"face_img.jpg\",\n",
    "    \"face_img_fbox.txt\",\n",
    "]\n",
    "\n",
    "out_dir = Path(\"./assets\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "resolved = {}\n",
    "for name in ASSET_NAMES:\n",
    "    src_path = CachedWebModelAsset.from_asset_store(\n",
    "        MODEL_ID, MODEL_ASSET_VERSION, name\n",
    "    ).fetch()\n",
    "    dst_path = out_dir / name\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    resolved[name] = str(dst_path)\n",
    "\n",
    "print(resolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495415cb",
   "metadata": {},
   "source": [
    "# Import utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610ea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETS_DIR = \"./assets\"\n",
    "\n",
    "\n",
    "def _load_assets(assets_dir=ASSETS_DIR):\n",
    "    face = np.load(Path(assets_dir) / \"meanFace.npy\").reshape(-1, 1).astype(np.float32)\n",
    "    basis_id = np.load(Path(assets_dir) / \"shapeBasis.npy\").astype(np.float32)\n",
    "    basis_exp = np.load(Path(assets_dir) / \"blendShape.npy\").astype(np.float32)\n",
    "    vn = 68\n",
    "    face = face.reshape(3 * vn, 1)\n",
    "    basis_id = basis_id.reshape(3 * vn, 219)\n",
    "    basis_exp = basis_exp.reshape(3 * vn, 39)\n",
    "    return face, basis_id, basis_exp, vn\n",
    "\n",
    "\n",
    "def _project(output, face, basis_id, basis_exp, vn):\n",
    "    a_id = output[0:219]\n",
    "    a_exp = output[219:258]\n",
    "    pitch = output[258]\n",
    "    yaw = output[259]\n",
    "    roll = output[260]\n",
    "    tX = output[261]\n",
    "    tY = output[262]\n",
    "    f = output[263]\n",
    "    a_id = a_id * 3.0\n",
    "    a_exp = a_exp * 0.5 + 0.5\n",
    "    pitch = pitch * np.pi / 2.0\n",
    "    yaw = yaw * np.pi / 2.0\n",
    "    roll = roll * np.pi / 2.0\n",
    "    tX = tX * 60.0\n",
    "    tY = tY * 60.0\n",
    "    tZ = 500.0\n",
    "    f = f * 150.0 + 450.0\n",
    "    p = np.array(\n",
    "        [\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [0.0, np.cos(-np.pi), -np.sin(-np.pi)],\n",
    "            [0.0, np.sin(-np.pi), np.cos(-np.pi)],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    cr, sr = np.cos(-roll), np.sin(-roll)\n",
    "    cp, sp = np.cos(-pitch), np.sin(-pitch)\n",
    "    cy, sy = np.cos(-yaw), np.sin(-yaw)\n",
    "    Rz = np.array([[cr, -sr, 0.0], [sr, cr, 0.0], [0.0, 0.0, 1.0]], dtype=np.float32)\n",
    "    Ry = np.array([[cy, 0.0, sy], [0.0, 1.0, 0.0], [-sy, 0.0, cy]], dtype=np.float32)\n",
    "    Rx = np.array([[1.0, 0.0, 0.0], [0.0, cp, -sp], [0.0, sp, cp]], dtype=np.float32)\n",
    "    R = Ry @ (Rx @ (p @ Rz))\n",
    "    shape = face + basis_id @ a_id.reshape(-1, 1) + basis_exp @ a_exp.reshape(-1, 1)\n",
    "    shape = shape.reshape(vn, 3)\n",
    "    V = shape @ R.T\n",
    "    V[:, 0] += tX\n",
    "    V[:, 1] += tY\n",
    "    V[:, 2] += tZ\n",
    "    lm = V[:, :2] * (f / tZ)\n",
    "    return lm.astype(np.float32), float(pitch), float(yaw), float(roll)\n",
    "\n",
    "\n",
    "def _normalize(inp, mode):\n",
    "    if mode == \"0_1\":\n",
    "        return inp.astype(np.float32) / 255.0\n",
    "    if mode == \"neg1_1\":\n",
    "        return (inp.astype(np.float32) / 127.5) - 1.0\n",
    "    return inp.astype(np.float32)\n",
    "\n",
    "\n",
    "def _rect_to_square(x0, y0, x1, y1, H, W, scale=1.1):\n",
    "    cx = 0.5 * (x0 + x1)\n",
    "    cy = 0.5 * (y0 + y1)\n",
    "    side = max(x1 - x0 + 1, y1 - y0 + 1) * scale\n",
    "    nx0 = int(round(cx - side / 2))\n",
    "    ny0 = int(round(cy - side / 2))\n",
    "    nx1 = int(round(cx + side / 2))\n",
    "    ny1 = int(round(cy + side / 2))\n",
    "    nx0 = max(0, nx0)\n",
    "    ny0 = max(0, ny0)\n",
    "    nx1 = min(W - 1, nx1)\n",
    "    ny1 = min(H - 1, ny1)\n",
    "    return nx0, ny0, nx1, ny1\n",
    "\n",
    "\n",
    "def _prep(\n",
    "    img,\n",
    "    bbox,\n",
    "    ih,\n",
    "    iw,\n",
    "    bbox_order=\"x0y0x1y1\",\n",
    "    square_crop=True,\n",
    "    square_scale=1.1,\n",
    "    norm=\"0_1\",\n",
    "    to_rgb=True,\n",
    "):\n",
    "    H, W = img.shape[:2]\n",
    "    if bbox is None:\n",
    "        x0, y0, x1, y1 = 0, 0, W - 1, H - 1\n",
    "    else:\n",
    "        if bbox_order == \"x0x1y0y1\":\n",
    "            bx0, bx1, by0, by1 = bbox\n",
    "            x0, y0, x1, y1 = int(bx0), int(by0), int(bx1), int(by1)\n",
    "        else:\n",
    "            x0, y0, x1, y1 = [int(v) for v in bbox]\n",
    "    if square_crop:\n",
    "        x0, y0, x1, y1 = _rect_to_square(x0, y0, x1, y1, H, W, square_scale)\n",
    "    x0 = max(0, x0)\n",
    "    y0 = max(0, y0)\n",
    "    x1 = min(W - 1, x1)\n",
    "    y1 = min(H - 1, y1)\n",
    "    roi = img[y0 : y1 + 1, x0 : x1 + 1]\n",
    "    if to_rgb:\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    roi = cv2.resize(roi, (iw, ih), interpolation=cv2.INTER_LINEAR)\n",
    "    inp = _normalize(roi, norm)[np.newaxis, ...]\n",
    "    return inp, (x0, y0, x1, y1)\n",
    "\n",
    "\n",
    "def _transform(lm, bbox, rh, rw):\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    h = (y1 - y0) + 1\n",
    "    w = (x1 - x0) + 1\n",
    "    out = lm.copy()\n",
    "    out[:, 0] = (out[:, 0] + rw / 2.0) * (w / rw) + x0\n",
    "    out[:, 1] = (out[:, 1] + rh / 2.0) * (h / rh) + y0\n",
    "    return out\n",
    "\n",
    "\n",
    "def _draw_pose_text(vis, bbox, deg):\n",
    "    x0, y0, x1, y1 = bbox\n",
    "    w = (x1 - x0) + 1\n",
    "    h = (y1 - y0) + 1\n",
    "    scale = max(0.5, 0.001 * max(w, h))\n",
    "    thick = max(1, int(round(scale * 2)))\n",
    "    text = f\"pitch:{deg[0]:.1f} yaw:{deg[1]:.1f} roll:{deg[2]:.1f}\"\n",
    "    (tw, th), base = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, scale, thick)\n",
    "    pad = max(2, int(round(0.2 * th)))\n",
    "    tx = max(0, min(x0, vis.shape[1] - tw - pad))\n",
    "    ty = min(vis.shape[0] - pad, y0 + th + pad)\n",
    "    overlay = vis.copy()\n",
    "    cv2.rectangle(\n",
    "        overlay,\n",
    "        (tx - pad, ty - th - pad),\n",
    "        (tx + tw + pad, ty + base + pad),\n",
    "        (0, 0, 0),\n",
    "        -1,\n",
    "    )\n",
    "    vis = cv2.addWeighted(overlay, 0.5, vis, 0.5, 0)\n",
    "    cv2.putText(\n",
    "        vis,\n",
    "        text,\n",
    "        (tx, ty),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        scale,\n",
    "        (0, 255, 0),\n",
    "        thick,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "    return vis\n",
    "\n",
    "\n",
    "def annotate_image(\n",
    "    image_path,\n",
    "    interpreter,\n",
    "    assets_dir=ASSETS_DIR,\n",
    "    bbox=None,\n",
    "    save_path=None,\n",
    "    bbox_order=\"x0y0x1y1\",\n",
    "    square_crop=True,\n",
    "    square_scale=1.1,\n",
    "    norm=\"0_1\",\n",
    "    to_rgb=True,\n",
    "    radius=10,\n",
    "    thickness=-1,\n",
    "):\n",
    "    face, basis_id, basis_exp, vn = _load_assets(assets_dir)\n",
    "    inp_info = interpreter.get_input_details()[0]\n",
    "    out_info = interpreter.get_output_details()[0]\n",
    "    ih, iw = int(inp_info[\"shape\"][1]), int(inp_info[\"shape\"][2])\n",
    "    img = cv2.imread(image_path)\n",
    "    inp, bbox_xyxy = _prep(\n",
    "        img, bbox, ih, iw, bbox_order, square_crop, square_scale, norm, to_rgb\n",
    "    )\n",
    "    interpreter.set_tensor(inp_info[\"index\"], inp.astype(np.float32))\n",
    "    interpreter.invoke()\n",
    "    out = interpreter.get_tensor(out_info[\"index\"])[0]\n",
    "    lm_crop, pitch, yaw, roll = _project(out, face, basis_id, basis_exp, vn)\n",
    "    lm_img = _transform(lm_crop, bbox_xyxy, ih, iw)\n",
    "    deg = np.array([pitch, yaw, roll]) * (180.0 / np.pi)\n",
    "    vis = img.copy()\n",
    "    vis = _draw_pose_text(vis, bbox_xyxy, deg)\n",
    "    if radius <= 0:\n",
    "        bw = (bbox_xyxy[2] - bbox_xyxy[0]) + 1\n",
    "        bh = (bbox_xyxy[3] - bbox_xyxy[1]) + 1\n",
    "        radius = max(3, int(0.008 * max(bw, bh)))\n",
    "    for x, y in lm_img:\n",
    "        cv2.circle(vis, (int(round(x)), int(round(y))), radius, (0, 255, 0), thickness)\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite(save_path, vis)\n",
    "    return lm_img, {\n",
    "        \"pitch_rad\": pitch,\n",
    "        \"yaw_rad\": yaw,\n",
    "        \"roll_rad\": roll,\n",
    "        \"pitch_deg\": deg[0],\n",
    "        \"yaw_deg\": deg[1],\n",
    "        \"roll_deg\": deg[2],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf4c1e",
   "metadata": {},
   "source": [
    "# Create adapter class for CoreML inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcda6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreMLInterpreterAdapter:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mlmodel,\n",
    "        input_name=\"input\",\n",
    "        output_name=\"output\",\n",
    "        expects_rgb=True,\n",
    "        input_range=\"uint8\",\n",
    "    ):\n",
    "        self.mlmodel = mlmodel\n",
    "        self.input_name = input_name\n",
    "        self.output_name = output_name\n",
    "        self.expects_rgb = expects_rgb\n",
    "        self.input_range = input_range\n",
    "        self._in = None\n",
    "        self._out = None\n",
    "        spec = self.mlmodel.get_spec()\n",
    "        in0 = spec.description.input[0]\n",
    "        self._shape = np.array(\n",
    "            [1, in0.type.imageType.height, in0.type.imageType.width, 3]\n",
    "        )\n",
    "\n",
    "    def get_input_details(self):\n",
    "        return [{\"name\": self.input_name, \"index\": 0, \"shape\": self._shape}]\n",
    "\n",
    "    def _infer_out_shape(self):\n",
    "        ih, iw = int(self._shape[1]), int(self._shape[2])\n",
    "        z = Image.fromarray(np.zeros((ih, iw, 3), np.uint8))\n",
    "        y = np.array(self.mlmodel.predict({self.input_name: z})[self.output_name])\n",
    "        return np.array([1, int(y.size)])\n",
    "\n",
    "    def get_output_details(self):\n",
    "        if not hasattr(self, \"_out_shape\"):\n",
    "            self._out_shape = self._infer_out_shape()\n",
    "        return [{\"name\": self.output_name, \"index\": 0, \"shape\": self._out_shape}]\n",
    "\n",
    "    def set_tensor(self, index, arr):\n",
    "        self._in = arr\n",
    "\n",
    "    def invoke(self):\n",
    "        x = self._in\n",
    "        if x is None:\n",
    "            raise RuntimeError(\"set_tensor must be called before invoke\")\n",
    "        x = np.squeeze(x, axis=0)\n",
    "        if x.dtype != np.uint8:\n",
    "            if self.input_range == \"uint8\":\n",
    "                x = np.clip(x, 0.0, 1.0) * 255.0\n",
    "            x = x.astype(np.uint8)\n",
    "        if x.shape[-1] == 3 and not self.expects_rgb:\n",
    "            x = x[..., ::-1]\n",
    "        pil = Image.fromarray(x)\n",
    "        pred = self.mlmodel.predict({self.input_name: pil})\n",
    "        y = np.array(pred[self.output_name], dtype=np.float32).reshape(1, -1)\n",
    "        self._out = y\n",
    "\n",
    "    def get_tensor(self, index):\n",
    "        if self._out is None:\n",
    "            raise RuntimeError(\"invoke must be called before get_tensor\")\n",
    "        return self._out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94098ae4",
   "metadata": {},
   "source": [
    "# Initialize TFLite Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ede8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded and tensors allocated successfully.\n",
      "\n",
      "--- Model Details ---\n",
      "Inputs:\n",
      " {'name': 'image', 'index': 0, 'shape': array([  1, 128, 128,   3], dtype=int32), 'shape_signature': array([  1, 128, 128,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "Outputs:\n",
      " {'name': 'parameters_3dmm', 'index': 85, 'shape': array([  1, 265], dtype=int32), 'shape_signature': array([  1, 265], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "---------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"models/LandmarkDetectionModel.tflite\"\n",
    "SAMPLE_DIR = \"sample\"\n",
    "RESULTS_DIR = \"results\"\n",
    "CONF_THRESH = 0.5\n",
    "NUM_LANDMARKS = 68\n",
    "os.makedirs(f\"tflite-{RESULTS_DIR}\", exist_ok=True)\n",
    "os.makedirs(f\"coreml-{RESULTS_DIR}\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    tflite_interpreter = Interpreter(model_path=MODEL_PATH)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    print(\"‚úÖ Model loaded and tensors allocated successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Please ensure the model file is at the correct path:\", MODEL_PATH)\n",
    "    exit()\n",
    "\n",
    "input_details = tflite_interpreter.get_input_details()[0]\n",
    "output_details = tflite_interpreter.get_output_details()[0]\n",
    "\n",
    "INPUT_SHAPE = input_details[\"shape\"]\n",
    "INPUT_HEIGHT = INPUT_SHAPE[1]\n",
    "INPUT_WIDTH = INPUT_SHAPE[2]\n",
    "\n",
    "print(\"\\n--- Model Details ---\")\n",
    "print(\"Inputs:\\n\", input_details)\n",
    "print(\"Outputs:\\n\", output_details)\n",
    "print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e0c68",
   "metadata": {},
   "source": [
    "# Initialize CoreML Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88b7793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Details ---\n",
      "Inputs:\n",
      " {'name': 'input', 'index': 0, 'shape': array([  1, 128, 128,   3])}\n",
      "Outputs:\n",
      " {'name': 'output', 'index': 0, 'shape': array([  1, 265])}\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"models/LandmarkDetectionModel.mlpackage\"\n",
    "\n",
    "try:\n",
    "    mlmodel = ct.models.MLModel(model=MODEL_PATH)\n",
    "    coreml_interpreter = CoreMLInterpreterAdapter(\n",
    "        mlmodel,\n",
    "        input_name=\"input\",\n",
    "        output_name=\"output\",\n",
    "        expects_rgb=True,\n",
    "        input_range=\"uint8\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Please ensure the model file is at the correct path:\", MODEL_PATH)\n",
    "    exit()\n",
    "\n",
    "input_details = coreml_interpreter.get_input_details()[0]\n",
    "output_details = coreml_interpreter.get_output_details()[0]\n",
    "\n",
    "INPUT_SHAPE = input_details[\"shape\"]\n",
    "INPUT_HEIGHT = INPUT_SHAPE[1]\n",
    "INPUT_WIDTH = INPUT_SHAPE[2]\n",
    "\n",
    "print(\"\\n--- Model Details ---\")\n",
    "print(\"Inputs:\\n\", input_details)\n",
    "print(\"Outputs:\\n\", output_details)\n",
    "print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c17c8",
   "metadata": {},
   "source": [
    "# Fetch image files from sample directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca08a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑLoading sample images...\n",
      "‚úÖSample Images Loaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"üîÑLoading sample images...\")\n",
    "    image_files = [f for f in os.listdir(SAMPLE_DIR) if f.endswith(\".jpg\")]\n",
    "    if not image_files:\n",
    "        print(\"‚ö†Ô∏è No .jpg images found in the 'sample' directory.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: The directory '{SAMPLE_DIR}' was not found.\")\n",
    "    image_files = []\n",
    "finally:\n",
    "    print(\"‚úÖSample Images Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa56bb",
   "metadata": {},
   "source": [
    "# Run inference on each sample image (.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560e7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [p for p in Path(SAMPLE_DIR).glob(\"*.jpg\")]\n",
    "for p in paths:\n",
    "    out_path = str(Path(f\"tflite-{RESULTS_DIR}\") / (p.stem + \"_annotated.jpg\"))\n",
    "    tflite_result = annotate_image(\n",
    "        str(p), tflite_interpreter, assets_dir=ASSETS_DIR, bbox=None, save_path=out_path\n",
    "    )\n",
    "    out_path = str(Path(f\"coreml-{RESULTS_DIR}\") / (p.stem + \"_annotated.jpg\"))\n",
    "    coreml_result = annotate_image(\n",
    "        str(p), coreml_interpreter, assets_dir=ASSETS_DIR, bbox=None, save_path=out_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcb0955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[126.84751, 197.22092],\n",
       "        [126.02574, 232.8127 ],\n",
       "        [128.57668, 265.71866],\n",
       "        [134.9893 , 303.11694],\n",
       "        [146.13406, 327.9792 ],\n",
       "        [159.5923 , 346.98584],\n",
       "        [176.73459, 361.64655],\n",
       "        [196.4888 , 374.2413 ],\n",
       "        [216.82167, 379.0624 ],\n",
       "        [237.51495, 377.39938],\n",
       "        [258.7813 , 367.97137],\n",
       "        [277.66373, 356.07495],\n",
       "        [293.48608, 339.30475],\n",
       "        [308.07635, 316.36453],\n",
       "        [319.83136, 280.3706 ],\n",
       "        [327.2766 , 248.2154 ],\n",
       "        [331.84674, 212.91841],\n",
       "        [159.69366, 188.71666],\n",
       "        [169.64903, 186.18839],\n",
       "        [180.50935, 185.65121],\n",
       "        [192.07141, 187.02788],\n",
       "        [201.98207, 190.04187],\n",
       "        [258.62445, 195.53204],\n",
       "        [268.6644 , 194.58078],\n",
       "        [280.19525, 195.42703],\n",
       "        [290.85797, 197.76718],\n",
       "        [300.45587, 201.6398 ],\n",
       "        [229.38399, 208.49495],\n",
       "        [227.99039, 227.62337],\n",
       "        [226.4346 , 248.9091 ],\n",
       "        [225.14165, 265.04727],\n",
       "        [200.06198, 276.2619 ],\n",
       "        [208.70294, 277.92508],\n",
       "        [223.66263, 281.2917 ],\n",
       "        [238.87036, 280.50677],\n",
       "        [247.56705, 280.2116 ],\n",
       "        [165.772  , 206.39055],\n",
       "        [179.21738, 199.88625],\n",
       "        [190.96405, 201.8792 ],\n",
       "        [200.74855, 211.91048],\n",
       "        [189.37418, 214.62329],\n",
       "        [178.26651, 213.92067],\n",
       "        [256.86346, 216.27132],\n",
       "        [268.0267 , 208.31343],\n",
       "        [279.90097, 208.2461 ],\n",
       "        [292.23083, 216.211  ],\n",
       "        [278.7626 , 221.68619],\n",
       "        [267.7004 , 220.66907],\n",
       "        [180.70018, 303.97266],\n",
       "        [192.89581, 300.2995 ],\n",
       "        [212.59665, 301.6424 ],\n",
       "        [221.74022, 304.00858],\n",
       "        [231.03227, 303.23193],\n",
       "        [250.52222, 305.11237],\n",
       "        [262.3687 , 310.7757 ],\n",
       "        [249.72565, 323.40063],\n",
       "        [231.28493, 327.004  ],\n",
       "        [220.44638, 326.31268],\n",
       "        [209.5852 , 325.25983],\n",
       "        [191.6901 , 318.70288],\n",
       "        [182.28223, 303.93536],\n",
       "        [210.42216, 309.73663],\n",
       "        [221.12497, 311.25256],\n",
       "        [231.83685, 311.5487 ],\n",
       "        [260.89996, 310.46817],\n",
       "        [231.82068, 312.93927],\n",
       "        [221.49646, 312.56732],\n",
       "        [211.19499, 311.25433]], dtype=float32),\n",
       " {'pitch_rad': -0.09141566757805993,\n",
       "  'yaw_rad': -0.002660497928989159,\n",
       "  'roll_rad': 0.0764114179965535,\n",
       "  'pitch_deg': -5.23773193359375,\n",
       "  'yaw_deg': -0.152435302734375,\n",
       "  'roll_deg': 4.3780517578125})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fbox = np.loadtxt(\"./assets/face_img_fbox.txt\")\n",
    "x0, x1, y0, y1 = [int(v) for v in fbox]\n",
    "bbox = (x0, y0, x1, y1)\n",
    "out_path = \"tflite-results/qcom_demo_check.jpg\"\n",
    "annotate_image(\n",
    "    \"./assets/face_img.jpg\",\n",
    "    tflite_interpreter,\n",
    "    bbox=bbox,\n",
    "    save_path=out_path,\n",
    ")\n",
    "out_path = \"coreml-results/qcom_demo_check.jpg\"\n",
    "annotate_image(\n",
    "    \"./assets/face_img.jpg\",\n",
    "    coreml_interpreter,\n",
    "    bbox=bbox,\n",
    "    save_path=out_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9755a",
   "metadata": {},
   "source": [
    "# Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da7a5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_params(\n",
    "    image_path,\n",
    "    interpreter,\n",
    "    assets_dir,\n",
    "    bbox=None,\n",
    "    bbox_order=\"x0y0x1y1\",\n",
    "    square_crop=True,\n",
    "    square_scale=1.1,\n",
    "    norm=\"0_1\",\n",
    "    to_rgb=True,\n",
    "):\n",
    "    face, basis_id, basis_exp, vn = _load_assets(assets_dir)\n",
    "    inp_info = interpreter.get_input_details()[0]\n",
    "    out_info = interpreter.get_output_details()[0]\n",
    "    ih, iw = int(inp_info[\"shape\"][1]), int(inp_info[\"shape\"][2])\n",
    "    img = cv2.imread(image_path)\n",
    "    inp, _ = _prep(\n",
    "        img, bbox, ih, iw, bbox_order, square_crop, square_scale, norm, to_rgb\n",
    "    )\n",
    "    interpreter.set_tensor(inp_info[\"index\"], inp.astype(np.float32))\n",
    "    interpreter.invoke()\n",
    "    out = interpreter.get_tensor(out_info[\"index\"])[0]\n",
    "    return out.ravel().astype(np.float64)\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a = a.ravel().astype(np.float64)\n",
    "    b = b.ravel().astype(np.float64)\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return np.nan\n",
    "    return float(np.dot(a, b) / (na * nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b6c829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pitch_rad': 0.048106609821965914, 'yaw_rad': -0.7189976459608002, 'roll_rad': 0.030785039486177133, 'pitch_deg': 2.7563057094812393, 'yaw_deg': -41.19553059339523, 'roll_deg': 1.763852834701538}\n",
      "{'pitch_rad': 0.4935035468493173, 'yaw_rad': -0.031622168024512336, 'roll_rad': -0.022954027304515394, 'pitch_deg': 28.275670409202576, 'yaw_deg': -1.811816766858101, 'roll_deg': -1.3151688873767853}\n",
      "{'pitch_rad': -0.03541107318476343, 'yaw_rad': -0.007352626266396744, 'roll_rad': -0.3743799533763554, 'pitch_deg': -2.028905041515827, 'yaw_deg': -0.42127445340156555, 'roll_deg': -21.4503912627697}\n",
      "{'pitch_rad': -0.1089539721531283, 'yaw_rad': 0.03182729254591486, 'roll_rad': 0.456609408277499, 'pitch_deg': -6.24260276556015, 'yaw_deg': 1.8235695362091064, 'roll_deg': 26.16179198026657}\n",
      "{'pitch_rad': 0.17187581083741468, 'yaw_rad': 0.7500865042736795, 'roll_rad': 0.029549107776089066, 'pitch_deg': 9.847758561372757, 'yaw_deg': 42.976790964603424, 'roll_deg': 1.6930391639471054}\n",
      "{'pitch_rad': -0.6436996841727385, 'yaw_rad': 0.021562136823832814, 'roll_rad': -0.023080259579739196, 'pitch_deg': -36.88127517700195, 'yaw_deg': 1.235419437289238, 'roll_deg': -1.3224014639854431}\n",
      "yaw-left.jpg: cos_sim=1.000000\n",
      "pitch-up.jpg: cos_sim=0.999999\n",
      "roll-left.jpg: cos_sim=1.000000\n",
      "roll-right.jpg: cos_sim=1.000000\n",
      "yaw-right.jpg: cos_sim=1.000000\n",
      "pitch-down.jpg: cos_sim=1.000000\n",
      "mean: 0.999999660592248\n",
      "min: 0.9999994380182312\n",
      "p5/p50/p95: [0.9999994741205619, 0.9999997154585478, 0.9999997589791395]\n",
      "yaw-left.jpg {'yawŒî': 0.0, 'pitchŒî': 0.01, 'rollŒî': 0.01}\n",
      "pitch-up.jpg {'yawŒî': 0.0, 'pitchŒî': 0.02, 'rollŒî': 0.01}\n",
      "roll-left.jpg {'yawŒî': 0.02, 'pitchŒî': 0.01, 'rollŒî': 0.01}\n",
      "roll-right.jpg {'yawŒî': 0.0, 'pitchŒî': 0.01, 'rollŒî': 0.01}\n",
      "yaw-right.jpg {'yawŒî': 0.0, 'pitchŒî': 0.02, 'rollŒî': 0.0}\n",
      "pitch-down.jpg {'yawŒî': 0.01, 'pitchŒî': 0.01, 'rollŒî': 0.0}\n"
     ]
    }
   ],
   "source": [
    "sims = []\n",
    "angle_deltas = []\n",
    "paths = [p for p in Path(SAMPLE_DIR).glob(\"*.jpg\")]\n",
    "for p in paths:\n",
    "    out_path = str(Path(f\"tflite-{RESULTS_DIR}\") / (p.stem + \"_annotated.jpg\"))\n",
    "    tflite_result = annotate_image(\n",
    "        str(p), tflite_interpreter, assets_dir=ASSETS_DIR, bbox=None, save_path=out_path\n",
    "    )\n",
    "    out_path = str(Path(f\"coreml-{RESULTS_DIR}\") / (p.stem + \"_annotated.jpg\"))\n",
    "    coreml_result = annotate_image(\n",
    "        str(p), coreml_interpreter, assets_dir=ASSETS_DIR, bbox=None, save_path=out_path\n",
    "    )\n",
    "\n",
    "    t_params = infer_params(str(p), tflite_interpreter, ASSETS_DIR)\n",
    "    c_params = infer_params(str(p), coreml_interpreter, ASSETS_DIR)\n",
    "    if t_params.shape != c_params.shape:\n",
    "        raise ValueError(\n",
    "            f\"Shape mismatch {t_params.shape} vs {c_params.shape} on {p.name}\"\n",
    "        )\n",
    "    sims.append((p.name, cosine_sim(t_params, c_params)))\n",
    "\n",
    "    _, t_pose = tflite_result\n",
    "    _, c_pose = coreml_result\n",
    "\n",
    "    print(t_pose)\n",
    "\n",
    "    angle_deltas.append(\n",
    "        (\n",
    "            p.name,\n",
    "            {\n",
    "                \"yawŒî\": (\n",
    "                    None\n",
    "                    if t_pose[\"yaw_deg\"] is None or c_pose[\"yaw_deg\"] is None\n",
    "                    else float(abs(t_pose[\"yaw_deg\"] - c_pose[\"yaw_deg\"]))\n",
    "                ),\n",
    "                \"pitchŒî\": (\n",
    "                    None\n",
    "                    if t_pose[\"pitch_deg\"] is None or c_pose[\"pitch_deg\"] is None\n",
    "                    else float(abs(t_pose[\"pitch_deg\"] - c_pose[\"pitch_deg\"]))\n",
    "                ),\n",
    "                \"rollŒî\": (\n",
    "                    None\n",
    "                    if t_pose[\"roll_deg\"] is None or c_pose[\"roll_deg\"] is None\n",
    "                    else float(abs(t_pose[\"roll_deg\"] - c_pose[\"roll_deg\"]))\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "for name, s in sims:\n",
    "    print(f\"{name}: cos_sim={s:.6f}\")\n",
    "\n",
    "vals = [s for _, s in sims if not np.isnan(s)]\n",
    "if vals:\n",
    "    print(\"mean:\", float(np.mean(vals)))\n",
    "    print(\"min:\", float(np.min(vals)))\n",
    "    print(\"p5/p50/p95:\", [float(np.percentile(vals, q)) for q in (5, 50, 95)])\n",
    "\n",
    "for name, d in angle_deltas:\n",
    "    print(name, {k: (None if v is None else round(v, 2)) for k, v in d.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
